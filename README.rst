CKIP Transformers
-----------------

| This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).
| é€™å€‹å°ˆæ¡ˆæä¾›äº†ç¹é«”ä¸­æ–‡çš„ transformers æ¨¡å‹ï¼ˆåŒ…å« ALBERTã€BERTã€GPT2ï¼‰åŠè‡ªç„¶èªè¨€è™•ç†å·¥å…·ï¼ˆåŒ…å«æ–·è©ã€è©æ€§æ¨™è¨˜ã€å¯¦é«”è¾¨è­˜ï¼‰ã€‚

Git
^^^

https://github.com/emfomy/ckip-transformers

|GitHub Version| |GitHub License| |GitHub Release| |GitHub Issues|

.. |GitHub Version| image:: https://img.shields.io/github/v/release/emfomy/ckip-transformers.svg?cacheSeconds=3600
   :target: https://github.com/emfomy/ckip-transformers/releases

.. |GitHub License| image:: https://img.shields.io/github/license/emfomy/ckip-transformers.svg?cacheSeconds=3600
   :target: https://github.com/emfomy/ckip-transformers/blob/master/LICENSE

.. |GitHub Release| image:: https://img.shields.io/github/release-date/emfomy/ckip-transformers.svg?cacheSeconds=3600

.. |GitHub Downloads| image:: https://img.shields.io/github/downloads/emfomy/ckip-transformers/total.svg?cacheSeconds=3600
   :target: https://github.com/emfomy/ckip-transformers/releases/latest

.. |GitHub Issues| image:: https://img.shields.io/github/issues/emfomy/ckip-transformers.svg?cacheSeconds=3600
   :target: https://github.com/emfomy/ckip-transformers/issues

.. |GitHub Forks| image:: https://img.shields.io/github/forks/emfomy/ckip-transformers.svg?style=social&label=Fork&cacheSeconds=3600

.. |GitHub Stars| image:: https://img.shields.io/github/stars/emfomy/ckip-transformers.svg?style=social&label=Star&cacheSeconds=3600

.. |GitHub Watchers| image:: https://img.shields.io/github/watchers/emfomy/ckip-transformers.svg?style=social&label=Watch&cacheSeconds=3600

PyPI
^^^^

https://pypi.org/project/ckip-transformers

|PyPI Version| |PyPI License| |PyPI Downloads| |PyPI Python| |PyPI Implementation| |PyPI Format| |PyPI Status|

.. |PyPI Version| image:: https://img.shields.io/pypi/v/ckip-transformers.svg?cacheSeconds=3600
   :target: https://pypi.org/project/ckip-transformers

.. |PyPI License| image:: https://img.shields.io/pypi/l/ckip-transformers.svg?cacheSeconds=3600
   :target: https://github.com/emfomy/ckip-transformers/blob/master/LICENSE

.. |PyPI Downloads| image:: https://img.shields.io/pypi/dm/ckip-transformers.svg?cacheSeconds=3600
   :target: https://pypi.org/project/ckip-transformers#files

.. |PyPI Python| image:: https://img.shields.io/pypi/pyversions/ckip-transformers.svg?cacheSeconds=3600

.. |PyPI Implementation| image:: https://img.shields.io/pypi/implementation/ckip-transformers.svg?cacheSeconds=3600

.. |PyPI Format| image:: https://img.shields.io/pypi/format/ckip-transformers.svg?cacheSeconds=3600

.. |PyPI Status| image:: https://img.shields.io/pypi/status/ckip-transformers.svg?cacheSeconds=3600

Documentation
^^^^^^^^^^^^^

https://ckip-transformers.readthedocs.io/

|ReadTheDocs Home|

.. |ReadTheDocs Home| image:: https://img.shields.io/website/https/ckip-transformers.readthedocs.io.svg?cacheSeconds=3600&up_message=online&down_message=offline
   :target: https://ckip-transformers.readthedocs.io

Relative Demos / Packages
^^^^^^^^^^^^^^^^^^^^^^^^^

- `CKIP Transformer Online Demo <https://ckip.iis.sinica.edu.tw/service/transformers/>`_: The online demo of this package.
- `CkipTagger <https://github.com/ckiplab/ckiptagger>`_: An alternative Chinese NLP library with using BiLSTM.
- `CKIP CoreNLP Toolkit <https://github.com/ckiplab/ckipnlp>`_: A Chinese NLP library with more NLP tasks and utilities.

Contributers
^^^^^^^^^^^^

* `Mu Yang <https://muyang.pro>`__ at `CKIP <https://ckip.iis.sinica.edu.tw>`__ (Author & Maintainer)
* `Wei-Yun Ma <https://www.iis.sinica.edu.tw/pages/ma/>`__ at `CKIP <https://ckip.iis.sinica.edu.tw>`__ (Maintainer)

Models
------

| One may also use our pretrained models with HuggingFace transformers library directly: https://huggingface.co/ckiplab/.
| æ‚¨å¯æ–¼ https://huggingface.co/ckiplab/ ä¸‹è¼‰é è¨“ç·´çš„æ¨¡å‹ã€‚

- Language Models

   * `ALBERT Tiny <https://huggingface.co/ckiplab/albert-tiny-chinese>`_: ``ckiplab/albert-tiny-chinese``
   * `ALBERT Base <https://huggingface.co/ckiplab/albert-base-chinese>`_: ``ckiplab/albert-base-chinese``
   * `BERT Base <https://huggingface.co/ckiplab/bert-base-chinese>`_: ``ckiplab/bert-base-chinese``
   * `GPT2 Base <https://huggingface.co/ckiplab/gpt2-base-chinese>`_: ``ckiplab/gpt2-base-chinese``

- NLP Task Models

   * `ALBERT Tiny â€” Word Segmentation <https://huggingface.co/ckiplab/albert-tiny-chinese-ws>`_: ``ckiplab/albert-tiny-chinese-ws``
   * `ALBERT Tiny â€” Part-of-Speech Tagging <https://huggingface.co/ckiplab/albert-tiny-chinese-pos>`_: ``ckiplab/albert-tiny-chinese-pos``
   * `ALBERT Tiny â€” Named-Entity Recognition <https://huggingface.co/ckiplab/albert-tiny-chinese-ner>`_: ``ckiplab/albert-tiny-chinese-ner``

   * `ALBERT Base â€” Word Segmentation <https://huggingface.co/ckiplab/albert-base-chinese-ws>`_: ``ckiplab/albert-base-chinese-ws``
   * `ALBERT Base â€” Part-of-Speech Tagging <https://huggingface.co/ckiplab/albert-base-chinese-pos>`_: ``ckiplab/albert-base-chinese-pos``
   * `ALBERT Base â€” Named-Entity Recognition <https://huggingface.co/ckiplab/albert-base-chinese-ner>`_: ``ckiplab/albert-base-chinese-ner``

   * `BERT Base â€” Word Segmentation <https://huggingface.co/ckiplab/bert-base-chinese-ws>`_: ``ckiplab/bert-base-chinese-ws``
   * `BERT Base â€” Part-of-Speech Tagging <https://huggingface.co/ckiplab/bert-base-chinese-pos>`_: ``ckiplab/bert-base-chinese-pos``
   * `BERT Base â€” Named-Entity Recognition <https://huggingface.co/ckiplab/bert-base-chinese-ner>`_: ``ckiplab/bert-base-chinese-ner``

Model Usage
^^^^^^^^^^^

| One may use our model directly from the ğŸ¤—/transformers library:
| æ‚¨å¯ç›´æ¥é€é ğŸ¤—/transformers å¥—ä»¶ä½¿ç”¨æˆ‘å€‘çš„æ¨¡å‹


.. code-block:: bash

   pip install -U transformers

| Please use BertTokenizerFast as tokenizer, and replace ``ckiplab/albert-tiny-chinese`` and ``ckiplab/albert-tiny-chinese-ws`` to any model you need in the following example.
| è«‹ä½¿ç”¨å…§å»ºçš„ BertTokenizerFastï¼Œä¸¦å°‡ä»¥ä¸‹ç¯„ä¾‹ä¸­çš„ ``ckiplab/albert-tiny-chinese`` èˆ‡ ``ckiplab/albert-tiny-chinese-ws`` æ›¿æ›æˆä»»ä½•ä½ è¦ä½¿ç”¨çš„æ¨¡å‹åç¨±ã€‚

.. code-block:: python

   from transformers import (
      BertTokenizerFast,
      AutoModelForMaskedLM,
      AutoModelForTokenClassification,
   )

   # language model
   tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')
   model = AutoModelForMaskedLM.from_pretrained('ckiplab/albert-tiny-chinese')

   # nlp task model
   tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')
   model = AutoModelForTokenClassification.from_pretrained('ckiplab/albert-tiny-chinese-ws')

Performance
^^^^^^^^^^^

| The following is a performance comparison between our model and other models.
| ä»¥ä¸‹æ˜¯æˆ‘å€‘çš„æ¨¡å‹èˆ‡å…¶ä»–çš„æ¨¡å‹ä¹‹æ€§èƒ½æ¯”è¼ƒã€‚

================================  ===========  ========  ==========  =========
Model                             Perplexityâ€   WS (F1)â€¡  POS (ACC)â€¡  NER (F1)â€¡
================================  ===========  ========  ==========  =========
ckiplab/albert-tiny-chinese        4.80        96.66%    94.48%      71.17%
ckiplab/albert-base-chinese        2.65        97.33%    95.30%      79.47%
ckiplab/bert-base-chinese          1.88        97.60%    95.67%      81.18%
ckiplab/gpt2-base-chinese         14.40        --        --          --
--------------------------------  -----------  --------  ----------  ---------

--------------------------------  -----------  --------  ----------  ---------
voidful/albert_chinese_tiny       74.93        --        --          --
voidful/albert_chinese_base       22.34        --        --          --
bert-base-chinese                  2.53        --        --          --
================================  ===========  ========  ==========  =========

| â€  Perplexity; the smaller the better.
| â€  æ··æ·†åº¦ï¼›æ•¸å­—è¶Šå°è¶Šå¥½ã€‚
| â€¡ WS: word segmentation; POS: part-of-speech; NER: named-entity recognition; the larger the better.
| â€¡ WS: æ–·è©ï¼›POS: è©æ€§æ¨™è¨˜ï¼›NER: å¯¦é«”è¾¨è­˜ï¼›æ•¸å­—è¶Šå¤§è¶Šå¥½ã€‚

NLP Tools
---------

| The package also provide the following NLP tools.
| æˆ‘å€‘çš„å¥—ä»¶ä¹Ÿæä¾›äº†ä»¥ä¸‹çš„è‡ªç„¶èªè¨€è™•ç†å·¥å…·ã€‚

* (WS) Word Segmentation æ–·è©
* (POS) Part-of-Speech Tagging è©æ€§æ¨™è¨˜
* (NER) Named Entity Recognition å¯¦é«”è¾¨è­˜

Installation
^^^^^^^^^^^^

``pip install -U ckip-transformers``

Requirements:

* `Python <https://www.python.org>`__ 3.6+
* `PyTorch <https://pytorch.org>`__ 1.1+
* `HuggingFace Transformers <https://huggingface.co/transformers/>`__ 3.5+

NLP Tools Usage
^^^^^^^^^^^^^^^

See https://ckip-transformers.readthedocs.io/en/latest/_api/ckip_transformers.html for API details.

The complete script of this example is https://github.com/ckiplab/ckip-transformers/blob/master/example/example.py.

1. Import module
""""""""""""""""

.. code-block:: python

   from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker

2. Load models
""""""""""""""

.. code-block:: python

   # Initialize drivers
   ws_driver  = CkipWordSegmenter()
   pos_driver = CkipPosTagger()
   ner_driver = CkipNerChunker()

3. Run pipeline
"""""""""""""""

- The input for word segmentation and named-entity recognition must be a list of sentences.
- The input for part-of-speech tagging must be a list of list of words (the output of word segmentation).

.. code-block:: python

   # Input text
   text = [
      'å‚…é”ä»ä»Šå°‡åŸ·è¡Œå®‰æ¨‚æ­»ï¼Œå»çªç„¶çˆ†å‡ºè‡ªå·±20å¹´å‰é­ç·¯ä¾†é«”è‚²å°å°æ®ºï¼Œä»–ä¸æ‡‚è‡ªå·±å“ªè£¡å¾—ç½ªåˆ°é›»è¦–å°ã€‚',
      'ç¾åœ‹åƒè­°é™¢é‡å°ä»Šå¤©ç¸½çµ±å¸ƒä»€æ‰€æåçš„å‹å·¥éƒ¨é•·è¶™å°è˜­å±•é–‹èªå¯è½è­‰æœƒï¼Œé æ–™å¥¹å°‡æœƒå¾ˆé †åˆ©é€šéåƒè­°é™¢æ”¯æŒï¼Œæˆç‚ºè©²åœ‹æœ‰å²ä»¥ä¾†ç¬¬ä¸€ä½çš„è¯è£”å¥³æ€§å…§é–£æˆå“¡ã€‚',
      'â€¦ ä½ ç¢ºå®šå—â€¦ ä¸è¦å†é¨™äº†â€¦â€¦',
   ]

   # Run pipeline
   ws  = ws_driver(text)
   pos = pos_driver(ws)
   ner = ner_driver(text)

4. Show results
"""""""""""""""

.. code-block:: python

   # Pack word segmentation and part-of-speech results
   def pack_ws_pos_sentece(sentence_ws, sentence_pos):
      assert len(sentence_ws) == len(sentence_pos)
      res = []
      for word_ws, word_pos in zip(sentence_ws, sentence_pos):
         res.append(f'{word_ws}({word_pos})')
      return '\u3000'.join(res)

   # Show results
   for sentence, sentence_ws, sentence_pos, sentence_ner in zip(text, ws, pos, ner):
      print(sentence)
      print(pack_ws_pos_sentece(sentence_ws, sentence_pos))
      for entity in sentence_ner:
         print(entity)
      print()

.. code-block:: text

   å‚…é”ä»ä»Šå°‡åŸ·è¡Œå®‰æ¨‚æ­»ï¼Œå»çªç„¶çˆ†å‡ºè‡ªå·±20å¹´å‰é­ç·¯ä¾†é«”è‚²å°å°æ®ºï¼Œä»–ä¸æ‡‚è‡ªå·±å“ªè£¡å¾—ç½ªåˆ°é›»è¦–å°ã€‚
   å‚…é”ä»(Nb)ã€€ä»Š(Nd)ã€€å°‡(D)ã€€åŸ·è¡Œ(VC)ã€€å®‰æ¨‚æ­»(Na)ã€€ï¼Œ(COMMACATEGORY)ã€€å»(D)ã€€çªç„¶(D)ã€€çˆ†å‡º(VJ)ã€€è‡ªå·±(Nh)ã€€20(Neu)ã€€å¹´(Nd)ã€€å‰(Ng)ã€€é­(P)ã€€ç·¯ä¾†(Nb)ã€€é«”è‚²å°(Na)ã€€å°æ®º(VC)ã€€ï¼Œ(COMMACATEGORY)ã€€ä»–(Nh)ã€€ä¸(D)ã€€æ‡‚(VK)ã€€è‡ªå·±(Nh)ã€€å“ªè£¡(Ncd)ã€€å¾—ç½ªåˆ°(VC)ã€€é›»è¦–å°(Nc)ã€€ã€‚(PERIODCATEGORY)
   NerToken(word='å‚…é”ä»', ner='PERSON', idx=(0, 3))
   NerToken(word='20å¹´', ner='DATE', idx=(18, 21))
   NerToken(word='ç·¯ä¾†é«”è‚²å°', ner='ORG', idx=(23, 28))

   ç¾åœ‹åƒè­°é™¢é‡å°ä»Šå¤©ç¸½çµ±å¸ƒä»€æ‰€æåçš„å‹å·¥éƒ¨é•·è¶™å°è˜­å±•é–‹èªå¯è½è­‰æœƒï¼Œé æ–™å¥¹å°‡æœƒå¾ˆé †åˆ©é€šéåƒè­°é™¢æ”¯æŒï¼Œæˆç‚ºè©²åœ‹æœ‰å²ä»¥ä¾†ç¬¬ä¸€ä½çš„è¯è£”å¥³æ€§å…§é–£æˆå“¡ã€‚
   ç¾åœ‹(Nc)ã€€åƒè­°é™¢(Nc)ã€€é‡å°(P)ã€€ä»Šå¤©(Nd)ã€€ç¸½çµ±(Na)ã€€å¸ƒä»€(Nb)ã€€æ‰€(D)ã€€æå(VC)ã€€çš„(DE)ã€€å‹å·¥éƒ¨é•·(Na)ã€€è¶™å°è˜­(Nb)ã€€å±•é–‹(VC)ã€€èªå¯(VC)ã€€è½è­‰æœƒ(Na)ã€€ï¼Œ(COMMACATEGORY)ã€€é æ–™(VE)ã€€å¥¹(Nh)ã€€å°‡(D)ã€€æœƒ(D)ã€€å¾ˆ(Dfa)ã€€é †åˆ©(VH)ã€€é€šé(VC)ã€€åƒè­°é™¢(Nc)ã€€æ”¯æŒ(VC)ã€€ï¼Œ(COMMACATEGORY)ã€€æˆç‚º(VG)ã€€è©²(Nes)ã€€åœ‹(Nc)ã€€æœ‰å²ä»¥ä¾†(D)ã€€ç¬¬ä¸€(Neu)ã€€ä½(Nf)ã€€çš„(DE)ã€€è¯è£”(Na)ã€€å¥³æ€§(Na)ã€€å…§é–£(Na)ã€€æˆå“¡(Na)ã€€ã€‚(PERIODCATEGORY)
   NerToken(word='ç¾åœ‹åƒè­°é™¢', ner='ORG', idx=(0, 5))
   NerToken(word='ä»Šå¤©', ner='LOC', idx=(7, 9))
   NerToken(word='å¸ƒä»€', ner='PERSON', idx=(11, 13))
   NerToken(word='å‹å·¥éƒ¨é•·', ner='ORG', idx=(17, 21))
   NerToken(word='è¶™å°è˜­', ner='PERSON', idx=(21, 24))
   NerToken(word='èªå¯è½è­‰æœƒ', ner='EVENT', idx=(26, 31))
   NerToken(word='åƒè­°é™¢', ner='ORG', idx=(42, 45))
   NerToken(word='ç¬¬ä¸€', ner='ORDINAL', idx=(56, 58))
   NerToken(word='è¯è£”', ner='NORP', idx=(60, 62))

   â€¦ ä½ ç¢ºå®šå—â€¦ ä¸è¦å†é¨™äº†â€¦â€¦
   â€¦(DASHCATEGORY)ã€€ (WHITESPACE)ã€€ä½ (Nh)ã€€ç¢ºå®š(VK)ã€€å—(T)ã€€â€¦(DASHCATEGORY)ã€€ (WHITESPACE)ã€€ä¸è¦(D)ã€€å†(D)ã€€é¨™(VC)ã€€äº†(Di)ã€€â€¦(DASHCATEGORY)ã€€â€¦(ETCCATEGORY)

Performance
^^^^^^^^^^^

| The following is a performance comparison between our tool and other tools.
| ä»¥ä¸‹æ˜¯æˆ‘å€‘çš„å·¥å…·èˆ‡å…¶ä»–çš„å·¥å…·ä¹‹æ€§èƒ½æ¯”è¼ƒã€‚

CKIP Transformers v.s. Monpa & Jeiba
""""""""""""""""""""""""""""""""""""

================================  ===========  =============  ===============  ============
Tool                                WS (F1)      POS (Acc)      WS+POS (F1)      NER (F1)
================================  ===========  =============  ===============  ============
CKIP BERT Base                    **97.60%**   **95.67%**     **94.19%**       **81.18%**
CKIP ALBERT Base                    97.33%       95.30%         93.52%           79.47%
CKIP ALBERT Tiny                    96.66%       94.48%         92.25%           71.17%
--------------------------------  -----------  -------------  ---------------  ------------

--------------------------------  -----------  -------------  ---------------  ------------
Monpaâ€                              92.58%       --             83.88%           21.51%
Jeiba                              81.18%       --             --              --
================================  ===========  =============  ===============  ============

| â€  Monpa provides only 3 types of tags in NER.
| â€  Monpa çš„å¯¦é«”è¾¨è­˜åƒ…æä¾›ä¸‰ç¨®æ¨™è¨˜è€Œå·²ã€‚

CKIP Transformers v.s. CkipTagger
""""""""""""""""""""""""""""""""""""

| The following results are tested on a different datasetã€‚
| ä»¥ä¸‹å¯¦é©—åœ¨å¦ä¸€å€‹è³‡æ–™é›†æ¸¬è©¦ã€‚

================================  ===========  =============  ===============  ============
Tool                                WS (F1)      POS (Acc)      WS+POS (F1)      NER (F1)
================================  ===========  =============  ===============  ============
CKIP BERT Base                    **97.84%**     96.46%       **94.91%**         79.20%
CkipTagger                          97.33%     **97.20%**       94.75%         **77.87%**
================================  ===========  =============  ===============  ============

License
-------

|GPL-3.0|

Copyright (c) 2020 `CKIP Lab <https://ckip.iis.sinica.edu.tw>`__ under the `GPL-3.0 License <https://www.gnu.org/licenses/gpl-3.0.html>`__.

.. |GPL-3.0| image:: https://www.gnu.org/graphics/gplv3-with-text-136x68.png
   :target: https://www.gnu.org/licenses/gpl-3.0.html
